大语言模型的上下文长度 =
预设的提示词长度 1000
+短期记忆长度 2000
+长期记忆长度 2000
+用户原始体温query长度 2000
+工具调用/知识库检索占用的长度 10000
+大模型剩余可以生成的长度

gpt-4o-mini: 32k=32000个token
